{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "    \n",
    "build_dir = \"/workspace/finn/lenet\"\n",
    "\n",
    "MODEL_PREFIX = \"test_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d\n",
    "\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantLinear\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas_examples.bnn_pynq.models.common import CommonWeightQuant, CommonActQuant\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas_examples.bnn_pynq.models.tensor_norm import TensorNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "from brevitas.nn import QuantReLU\n",
    "from brevitas.core.quant import QuantType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNV_OUT_CH_POOL = [(64, False), (64, True), (128, False), (128, True), (256, False), (256, False)]\n",
    "CNV_OUT_CH_POOL = [(6, True), (16, True)]\n",
    "\n",
    "INTERMEDIATE_FC_FEATURES = [(16*4*4, 120), (120, 84)]\n",
    "LAST_FC_IN_FEATURES = 84\n",
    "\n",
    "POOL_SIZE = 2\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "MAX_RELU_VAL = 6.0\n",
    "\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes, weight_bit_width, act_bit_width, in_bit_width, in_ch=1):\n",
    "        super(CNV, self).__init__()\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        self.conv_features.append(QuantIdentity( # for Q1.7 input format\n",
    "            act_quant=CommonActQuant,\n",
    "            bit_width=in_bit_width,\n",
    "            min_val=- 1.0,\n",
    "            max_val=1.0 - 2.0 ** (-7),\n",
    "            narrow_range=False,\n",
    "            restrict_scaling_type=RestrictValueType.POWER_OF_TWO))\n",
    "\n",
    "        for out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(QuantConv2d(\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                in_channels=in_ch,\n",
    "                out_channels=out_ch,\n",
    "                bias=False,\n",
    "                weight_quant=CommonWeightQuant,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "            \n",
    "#             self.conv_features.append(QuantReLU(\n",
    "#                 bit_width=act_bit_width,\n",
    "#                 quant_type=QuantType.INT,\n",
    "#                 weight_quant=CommonWeightQuant\n",
    "#             ))\n",
    "            \n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm2d(in_ch, eps=1e-4))\n",
    "            self.conv_features.append(QuantIdentity(\n",
    "                act_quant=CommonActQuant,\n",
    "                bit_width=act_bit_width))\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool2d(kernel_size=2))\n",
    "\n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(QuantLinear(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                bias=False,\n",
    "                weight_quant=CommonWeightQuant,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "            \n",
    "#             self.linear_features.append(QuantReLU(\n",
    "#                 bit_width=act_bit_width,\n",
    "#                 quant_type=QuantType.INT,\n",
    "#                 weight_quant=CommonWeightQuant,\n",
    "#             ))\n",
    "            \n",
    "            self.linear_features.append(BatchNorm1d(out_features, eps=1e-4))\n",
    "            self.linear_features.append(QuantIdentity(\n",
    "                act_quant=CommonActQuant,\n",
    "                bit_width=act_bit_width))\n",
    "\n",
    "        self.linear_features.append(QuantLinear(\n",
    "            in_features=LAST_FC_IN_FEATURES,\n",
    "            out_features=num_classes,\n",
    "            bias=False,\n",
    "            weight_quant=CommonWeightQuant,\n",
    "            weight_bit_width=weight_bit_width))\n",
    "        \n",
    "#         self.linear_features.append(QuantReLU(\n",
    "#             bit_width=act_bit_width,\n",
    "#             quant_type=QuantType.INT,\n",
    "#             weight_quant=CommonWeightQuant,\n",
    "#             max_val=MAX_RELU_VAL,\n",
    "# #             scaling_impl_type=ScalingImplType.CONST,\n",
    "# #             scaling_stats_permute_dims=None,\n",
    "# #             scaling_stats_op=StatsOp.MAX\n",
    "#         ))\n",
    "        \n",
    "        self.linear_features.append(TensorNorm())\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d) or isinstance(m, QuantLinear):\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "\n",
    "\n",
    "    def clip_weights(self, min_val, max_val):\n",
    "        for mod in self.conv_features:\n",
    "            if isinstance(mod, QuantConv2d):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "        for mod in self.linear_features:\n",
    "            if isinstance(mod, QuantLinear):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def cnv(weight_bit_width, act_bit_width, in_bit_width, num_classes, in_channels):\n",
    "#     weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH')\n",
    "#     act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')\n",
    "#     in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')\n",
    "#     num_classes = cfg.getint('MODEL', 'NUM_CLASSES')\n",
    "#     in_channels = cfg.getint('MODEL', 'IN_CHANNELS')\n",
    "    net = CNV(weight_bit_width=weight_bit_width,\n",
    "              act_bit_width=act_bit_width,\n",
    "              in_bit_width=in_bit_width,\n",
    "              num_classes=num_classes,\n",
    "              in_ch=in_channels)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = cnv(4, 8, 8, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_export_debug_name',\n",
       " '_get_name',\n",
       " '_named_members',\n",
       " '_parameters',\n",
       " '_tracing_name',\n",
       " 'export_debug_name',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_parameter']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(model.conv_features[0]) if 'ame' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QuantIdentity',\n",
       " 'QuantConv2d',\n",
       " 'BatchNorm2d',\n",
       " 'QuantIdentity',\n",
       " 'MaxPool2d',\n",
       " 'QuantConv2d',\n",
       " 'BatchNorm2d',\n",
       " 'QuantIdentity',\n",
       " 'MaxPool2d']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x._get_name() for x in model.conv_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QuantLinear',\n",
       " 'BatchNorm1d',\n",
       " 'QuantIdentity',\n",
       " 'QuantLinear',\n",
       " 'BatchNorm1d',\n",
       " 'QuantIdentity',\n",
       " 'QuantLinear',\n",
       " 'TensorNorm']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x._get_name() for x in model.linear_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:103: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n"
     ]
    }
   ],
   "source": [
    "bo.export_finn_onnx(model, (1, 1, 28, 28), build_dir + f\"/{MODEL_PREFIX}_export.onnx\")\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_export.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(InferShapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(FoldConstants())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(GiveUniqueNodeNames())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(GiveReadableTensorNames())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(RemoveStaticGraphInputs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(build_dir + f\"/{MODEL_PREFIX}_tidy.onnx\")\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(build_dir+f\"/{MODEL_PREFIX}_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = build_dir+f\"/{MODEL_PREFIX}_preproc.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
