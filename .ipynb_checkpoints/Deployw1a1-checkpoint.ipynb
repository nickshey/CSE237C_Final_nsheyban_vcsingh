{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "\n",
    "import torch\n",
    "\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d\n",
    "from QuantLenetV2 import *\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantLinear\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas_examples.bnn_pynq.models.common import CommonWeightQuant, CommonActQuant\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas_examples.bnn_pynq.models.tensor_norm import TensorNorm\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "\n",
    "import netron\n",
    "\n",
    "stopit = lambda: netron.stop(8081, \"0.0.0.0\")\n",
    "\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "from brevitas.nn import QuantReLU\n",
    "from brevitas.core.quant import QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_WIDTH = 8\n",
    "WEIGHT_WIDTH = 1\n",
    "ACT_WIDTH = 1\n",
    "MODEL_PREFIX = f\"model_i{INPUT_WIDTH}_w{WEIGHT_WIDTH}_a{ACT_WIDTH}\"\n",
    "\n",
    "model = cnv(INPUT_WIDTH,WEIGHT_WIDTH,ACT_WIDTH,num_classes=10, in_channels=1)\n",
    "# from QuantLeNet import QuantLeNet\n",
    "# model = QuantLeNet(INPUT_WIDTH, WEIGHT_WIDTH, ACT_WIDTH)\n",
    "path=f\"./models/model_i{INPUT_WIDTH}_w{WEIGHT_WIDTH}_a{ACT_WIDTH}.pth\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn/notebooks/LeNet/QuantLenetV2.py:92: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x = 2.0 * x - torch.tensor([1.0], device=x.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './onnx/model_i8_w1_a1_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f43cd9b2cf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_dir = './onnx'\n",
    "\n",
    "bo.export_finn_onnx(model, (1, IN_CH, IMG_WIDTH, IMG_HEIGHT), build_dir + f\"/{MODEL_PREFIX}_export.onnx\")\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_export.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + f\"/{MODEL_PREFIX}_tidy.onnx\")\n",
    "\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn/src/finn/transformation/infer_data_layouts.py:113: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w1_a1_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f44486fce80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(build_dir+f\"/{MODEL_PREFIX}_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = build_dir+f\"/{MODEL_PREFIX}_preproc.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)\n",
    "\n",
    "from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = build_dir+f\"/{MODEL_PREFIX}_pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w1_a1_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f43cd9be6a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_pre_post.onnx\")\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "# quantization width greater than 1, so we don't do this\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(build_dir + f\"/{MODEL_PREFIX}_streamlined.onnx\")\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w1_a1_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f44486f6e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_streamlined.onnx\")\n",
    "\n",
    "# Not doing Binary Streaming FC Layer because we don't have a BNN\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# # input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# # get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# # get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "\n",
    "# model = model.transform(InferDataLayouts())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + f\"/{MODEL_PREFIX}_dataflow_parent.onnx\")\n",
    "showInNetron(build_dir + f\"/{MODEL_PREFIX}_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w1_a1_dataflow_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f43cd9cc0f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + f\"/{MODEL_PREFIX}_dataflow_model.onnx\")\n",
    "showInNetron(build_dir + f\"/{MODEL_PREFIX}_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w1_a1_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f43cda33278>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto-folding did not succeed. It resulted in an error further downstream\n",
    "# from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_dataflow_model.onnx\")\n",
    "\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "# print(len(fc_layers))\n",
    "\n",
    "# print (fc_layers[2])\n",
    "\n",
    "folding = [\n",
    "    (16, 1, 128),\n",
    "    (16, 16, 128),\n",
    "    (16, 16, 128),\n",
    "    (16, 16, 128),\n",
    "    (1, 16, 2),\n",
    "    (1, 4, 2),\n",
    "    (1, 8, 128),\n",
    "    (5, 1, 1),\n",
    "]\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "\n",
    "# folded_model = dataflow_model.transform(SetFolding())\n",
    "model.save(build_dir + f\"/{MODEL_PREFIX}_folded.onnx\")\n",
    "showInNetron(build_dir + f\"/{MODEL_PREFIX}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n"
     ]
    }
   ],
   "source": [
    "stopit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_pynq_board = \"Pynq-Z2\"\n",
    "target_clk_ns = 10\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(build_dir+ f\"/{MODEL_PREFIX}_folded.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = test_pynq_board, period_ns = target_clk_ns))\n",
    "model.save(build_dir +  f\"/{MODEL_PREFIX}_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "# set up the following values according to your own environment\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "ip = os.getenv(\"PYNQ_IP\", \"192.168.254.35\")\n",
    "username = os.getenv(\"PYNQ_USERNAME\", \"xilinx\")\n",
    "password = os.getenv(\"PYNQ_PASSWORD\", \"xilinx\")\n",
    "port = os.getenv(\"PYNQ_PORT\", 22)\n",
    "target_dir = os.getenv(\"PYNQ_TARGET_DIR\", \"/home/xilinx/finn_cnv_end2end_example\")\n",
    "ipAdd = \"192.168.254.35\"\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_synth.onnx\")\n",
    "model = model.transform(DeployToPYNQ(ipAdd, port, username, password, target_dir))\n",
    "model.save(build_dir + f\"/{MODEL_PREFIX}_pynqZ2_deploy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xilinx/finn_dev_nojan/pynq_deployment_emai1n7x'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dir_pynq = target_dir + \"/\" + model.get_metadata_prop(\"pynq_deployment_dir\").split(\"/\")[-1]\n",
    "target_dir_pynq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4224\r\n",
      "-rw-r--r-- 1 xilinx xilinx    8508 Dec 16 03:55 driver.py\r\n",
      "drwxr-xr-x 4 xilinx xilinx    4096 Dec 16 03:55 finn\r\n",
      "-rw-r--r-- 1 xilinx xilinx    3264 Dec 16 03:56 input.npy\r\n",
      "-rw-r--r-- 1 xilinx xilinx 4045671 Dec 16 03:55 resizer.bit\r\n",
      "-rw-r--r-- 1 xilinx xilinx  247307 Dec 16 03:55 resizer.hwh\r\n",
      "-rw-r--r-- 1 root   root        32 Dec 16 03:56 sds_trace_data.dat\r\n",
      "-rw-r--r-- 1 xilinx xilinx    1727 Dec 16 03:55 validate.py\r\n"
     ]
    }
   ],
   "source": [
    "! sshpass -p {password} ssh {username}@192.168.254.35 -p {port} 'ls -l {target_dir_pynq}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff31ccd9780>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pkgutil import get_data\n",
    "import onnx.numpy_helper as nph\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "DATASET_ROOT = \"/workspace/finn/src/data/fashion\"\n",
    "test_data = torchvision.datasets.FashionMNIST(DATASET_ROOT, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=50, shuffle=False)\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "testImage = images[8]\n",
    "plt.imshow(testImage.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected network input shape is [1, 28, 28, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sandal'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_pynqZ2_deploy.onnx\")\n",
    "iname = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(iname)\n",
    "print(\"Expected network input shape is \" + str(ishape))\n",
    "\n",
    "import numpy as np\n",
    "import bitstring\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "test = testImage[0].numpy()*255\n",
    "\n",
    "input_dict = {iname: test.reshape(ishape)}\n",
    "ret = execute_onnx(model, input_dict, False)\n",
    "\n",
    "classes = ('t-shirt/top', 'trouser', 'pullover', 'dress', \\\n",
    "           'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot')\n",
    "\n",
    "classes[int(ret[\"global_out\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network metrics:\n",
      "runtime[ms]: 971.9593524932861\n",
      "throughput[images/s]: 10288.496092298341\n",
      "DRAM_in_bandwidth[Mb/s]: 8.0661809363619\n",
      "DRAM_out_bandwidth[Mb/s]: 0.01028849609229834\n",
      "fclk[mhz]: 100.0\n",
      "N: 10000\n"
     ]
    }
   ],
   "source": [
    "from finn.core.throughput_test import throughput_test_remote\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_pynqZ2_deploy.onnx\")\n",
    "res = throughput_test_remote(model, 10000)\n",
    "print(\"Network metrics:\")\n",
    "for key in res:\n",
    "    print(str(key) + \": \" + str(res[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
