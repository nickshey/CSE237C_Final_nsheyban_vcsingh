{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d\n",
    "from QuantLeNet import QuantLeNet\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantLinear\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas_examples.bnn_pynq.models.common import CommonWeightQuant, CommonActQuant\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas_examples.bnn_pynq.models.tensor_norm import TensorNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_WIDTH = 8\n",
    "WEIGHT_WIDTH = 4\n",
    "ACT_WIDTH = 8\n",
    "BATCH_SIZE = 50\n",
    "    \n",
    "build_dir = \"./onnx\"\n",
    "MODEL_PREFIX = f\"model_i{INPUT_WIDTH}_w{WEIGHT_WIDTH}_a{ACT_WIDTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QuantLeNet(INPUT_WIDTH,WEIGHT_WIDTH,ACT_WIDTH)\n",
    "path=f\"./models/model_i{INPUT_WIDTH}_w{WEIGHT_WIDTH}_a{ACT_WIDTH}.pth\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w4_a8_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa2a4c4e080>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.export_finn_onnx(model, (1, 1, 28, 28), build_dir + f\"/{MODEL_PREFIX}_export.onnx\")\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_export.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + f\"/{MODEL_PREFIX}_tidy.onnx\")\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn/src/finn/transformation/infer_data_layouts.py:113: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(build_dir+f\"/{MODEL_PREFIX}_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = build_dir+f\"/{MODEL_PREFIX}_preproc.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w4_a8_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa2a9d4c6d8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = build_dir+f\"/{MODEL_PREFIX}_pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving './onnx/model_i8_w4_a8_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa2a4c4edd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_pre_post.onnx\")\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(build_dir + f\"/{MODEL_PREFIX}_streamlined.onnx\")\n",
    "showInNetron(build_dir+f\"/{MODEL_PREFIX}_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa27f6130f0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pkgutil import get_data\n",
    "import onnx.numpy_helper as nph\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "DATASET_ROOT = \"/workspace/finn/src/data/fashion\"\n",
    "test_data = torchvision.datasets.FashionMNIST(DATASET_ROOT, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "testImage = images[2]\n",
    "plt.imshow(testImage.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected network input shape is [1, 1, 28, 28]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trouser'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + f\"/{MODEL_PREFIX}_streamlined.onnx\")\n",
    "iname = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(iname)\n",
    "print(\"Expected network input shape is \" + str(ishape))\n",
    "\n",
    "import numpy as np\n",
    "import bitstring\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "test = testImage[0].numpy()*255\n",
    "\n",
    "input_dict = {iname: test.reshape(ishape)}\n",
    "ret = execute_onnx(model, input_dict)\n",
    "\n",
    "classes = ('t-shirt/top', 'trouser', 'pullover', 'dress', \\\n",
    "           'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot')\n",
    "\n",
    "classes[ret[\"global_out\"][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
